{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP9EnoSlSZAjxEmimfw4SPv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvaroEkel/AI-Spielplatz/blob/main/AlphaTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IZa8eq8zuaAt"
      },
      "outputs": [],
      "source": [
        "# !pip install torch numpy scipy statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "# import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import acf"
      ],
      "metadata": {
        "id": "dAqBmGV3urJC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesEmbeddingAlpha:\n",
        "    def __init__(self, time_series, dimension, lag):\n",
        "        self.time_series = np.array(time_series, dtype=float)\n",
        "        self.dimension = dimension\n",
        "        self.lag = lag\n",
        "        self.n_del_vec = len(self.time_series) - (dimension - 1) * lag\n",
        "        self.delay_vectors = self.compute_delay_vectors()\n",
        "        self.alpha_sequence = self.compute_alpha_sequence()\n",
        "\n",
        "    def compute_delay_vectors(self):\n",
        "        n = self.n_del_vec\n",
        "        delay_vectors = np.zeros((n, self.dimension))\n",
        "        for i in range(n):\n",
        "            for j in range(self.dimension):\n",
        "                delay_vectors[i, j] = self.time_series[i + j * self.lag]\n",
        "        return delay_vectors\n",
        "\n",
        "    def ordinal_rank_vectors(self):\n",
        "        return np.apply_along_axis(lambda row: np.argsort(np.argsort(row)), 1, self.delay_vectors) + 1\n",
        "\n",
        "    def alpha(self, ordinal_rank):\n",
        "        n = len(ordinal_rank)\n",
        "        lim = n // 2\n",
        "        tau_n = ordinal_rank[:lim]\n",
        "        tau_p = ordinal_rank[lim+1:] if n % 2 else ordinal_rank[lim:]\n",
        "        return np.sum(tau_p) - np.sum(tau_n)\n",
        "\n",
        "    def compute_alpha_sequence(self):\n",
        "        ordinal_ranks = self.ordinal_rank_vectors()\n",
        "        return np.array([self.alpha(ordinal_ranks[i, :]) for i in range(len(ordinal_ranks))], dtype=int)"
      ],
      "metadata": {
        "id": "Q50DXG4dur8Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "7sfbxPi_9-uD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677677c6-fc91-47c1-b990-565a0a91e2fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: change to specific directory and set as working directory\n",
        "\n",
        "import os\n",
        "\n",
        "# Change directory\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/AlphaTransformer/')\n",
        "\n",
        "# Verify the current working directory\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "wdnol8QBvYUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8b93d4-9c68-4eb6-ec22-aeda32124f20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/AlphaTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "commodity prices: corn, wheat, rice, soybean, soybean oil, lumber, copper..."
      ],
      "metadata": {
        "id": "tAfZ9ESfL36E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the column value from the file copper-prices-historical-chart-data in the current directory\n",
        "\n",
        "import pandas as pd\n",
        "print(os.getcwd())\n",
        "\n",
        "# Try different encodings:\n",
        "encodings_to_try = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']  # Add other encodings if needed\n",
        "\n",
        "for encoding in encodings_to_try:\n",
        "    try:\n",
        "        df = pd.read_csv('./data/corn-prices-historical-chart-data_clean-1.csv', encoding=encoding)\n",
        "        df = pd.read_csv('./data/corn-prices-historical-chart-data_clean-1.csv', encoding=encoding)\n",
        "        df.dropna(inplace=True)\n",
        "        print(df)\n",
        "        seq = df['value']\n",
        "        print(f\"Successfully loaded data using encoding: {encoding}\")\n",
        "        break  # Stop if successful\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"Failed to load data using encoding: {encoding}\")\n",
        "\n",
        "# remove the nan values form the seq list\n",
        "\n",
        "seq_array = seq.to_numpy()  # Convert to NumPy array\n",
        "seq_array"
      ],
      "metadata": {
        "id": "NRGjCcOvMDuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99e2bd0-c94f-434b-e851-d41d1de46796"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/AlphaTransformer\n",
            "             date   value\n",
            "0      1959-07-01  1.1770\n",
            "1      1959-07-02  1.1760\n",
            "2      1959-07-06  1.1710\n",
            "3      1959-07-07  1.1710\n",
            "4      1959-07-08  1.1700\n",
            "...           ...     ...\n",
            "16579  2025-03-12  4.4925\n",
            "16580  2025-03-13  4.5500\n",
            "16581  2025-03-14  4.4325\n",
            "16582  2025-03-17  4.6050\n",
            "16583  2025-03-18  4.6050\n",
            "\n",
            "[16584 rows x 2 columns]\n",
            "Successfully loaded data using encoding: utf-8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.177 , 1.176 , 1.171 , ..., 4.4325, 4.605 , 4.605 ])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def divide_into_subsequences(seq_array, subseq_length):\n",
        "    \"\"\"Divides a sequence into disjoint subsequences of specified length.\n",
        "\n",
        "    Args:\n",
        "        seq_array: The input sequence as a NumPy array.\n",
        "        subseq_length: The desired length of each subsequence.\n",
        "\n",
        "    Returns:\n",
        "        A list of NumPy arrays, each representing a disjoint subsequence.\n",
        "    \"\"\"\n",
        "\n",
        "    num_subsequences = len(seq_array) // subseq_length  # Calculate number of subsequences\n",
        "    subsequences = []\n",
        "    for i in range(num_subsequences):\n",
        "        start_index = i * subseq_length\n",
        "        end_index = (i + 1) * subseq_length\n",
        "        subsequence = seq_array[start_index:end_index]\n",
        "        subsequences.append(subsequence)\n",
        "\n",
        "    return subsequences\n",
        "sequence_length = 128  # Length\n",
        "# Example usage:\n",
        "# Assuming you have seq_array and subseq_length defined\n",
        "subsequences = divide_into_subsequences(seq_array, subseq_length=sequence_length)  # Example subseq_length\n",
        "len(subsequences)\n",
        "# Now subsequences is a list of NumPy arrays, each of length 64\n",
        "# You can access them like this:\n",
        "# first_subsequence = subsequences[0]\n",
        "# second_subsequence = subsequences[1]\n",
        "# ... and so on"
      ],
      "metadata": {
        "collapsed": true,
        "id": "18T3K-WC0MTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TimeSeriesEmbeddingAlpha to each subsequence\n",
        "dimension = 16\n",
        "lag = 1\n",
        "alpha_sequences = []\n",
        "for subsequence in subsequences:\n",
        "    embedding = TimeSeriesEmbeddingAlpha(subsequence, dimension, lag)\n",
        "    alpha_sequences.append(embedding.alpha_sequence)\n",
        "\n",
        "alpha_sequences = np.array(alpha_sequences)\n",
        "print(len(alpha_sequences))"
      ],
      "metadata": {
        "id": "vsj0ftGz25Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brownian paths"
      ],
      "metadata": {
        "id": "WgLMULdryHjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate  Brownian motion paths and compute their alpha sequences\n",
        "np.random.seed(42)\n",
        "num_samples = 20000\n",
        "sequence_length = 64  # Length\n",
        "dimension = 8\n",
        "lag = 1\n",
        "\n",
        "alpha_sequences = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    time_series_brownian = np.cumsum(np.random.normal(0, 1, sequence_length))\n",
        "    embedding = TimeSeriesEmbeddingAlpha(time_series, dimension, lag)\n",
        "    alpha_sequences.append(embedding.alpha_sequence)\n"
      ],
      "metadata": {
        "id": "5uVc3Mu0uT5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic map trajectories"
      ],
      "metadata": {
        "id": "M9KeTJxIyPzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate  Brownian motion paths and compute their alpha sequences\n",
        "np.random.seed(42)\n",
        "num_samples = 20000\n",
        "sequence_length = 64  # Length\n",
        "dimension = 8\n",
        "lag = 1\n",
        "\n",
        "alpha_sequences = []\n",
        "\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    time_series = [random.uniform(0, 1)]  # Random initial value\n",
        "    for _ in range(sequence_length - 1): time_series.append(4 * time_series[-1] * (1 - time_series[-1]))\n",
        "    embedding = TimeSeriesEmbeddingAlpha(time_series, dimension, lag)\n",
        "    alpha_sequences.append(embedding.alpha_sequence)\n",
        "\n",
        "alpha_sequences = np.array(alpha_sequences)"
      ],
      "metadata": {
        "id": "I3X9wQBayPKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get tokens"
      ],
      "metadata": {
        "id": "IaPHD4SvMiAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get unique values (sorted) and create a discrete token map\n",
        "unique_values = np.sort(np.unique(alpha_sequences))\n",
        "token_map = {value: idx for idx, value in enumerate(unique_values)}\n",
        "inverse_token_map = {idx: value for value, idx in token_map.items()}\n",
        "\n",
        "# Convert sequences to token indices\n",
        "alpha_sequences_tokenized = np.vectorize(token_map.get)(alpha_sequences)\n",
        "vocab_size = len(token_map)  # Number of unique tokens\n",
        "\n",
        "print(f\"Unique Alpha Values: {unique_values}\")\n",
        "print(f\"Token Mapping: {token_map}\")\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(f\"Example of tokenized sequence: {alpha_sequences_tokenized[0]}\")"
      ],
      "metadata": {
        "id": "9LjGCy7_uwbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values"
      ],
      "metadata": {
        "id": "d4sQ4F8JvD-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_map"
      ],
      "metadata": {
        "id": "5F7JtmhFvuva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define transformer model"
      ],
      "metadata": {
        "id": "5nrp3bJFMlNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForSequences(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers):\n",
        "        super(TransformerForSequences, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # Convert token indices into embeddings\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=ff_hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)  # Output probabilities for token indices\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = self.fc_out(output)  # Shape: (batch, seq_len, vocab_size)\n",
        "        return output\n",
        "\n",
        "# Model parameters\n",
        "embed_dim = 16\n",
        "num_heads = 8\n",
        "ff_hidden_dim = 16\n",
        "num_layers = 8\n",
        "\n",
        "# Create model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerForSequences(vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers).to(device)\n",
        "print(\"Model initialized and moved to:\", device)"
      ],
      "metadata": {
        "id": "sD_Cah-Cv6uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokenized sequences to PyTorch tensors\n",
        "X_train = torch.tensor(alpha_sequences_tokenized[:, :-1], dtype=torch.long).to(device)  # Input sequences\n",
        "y_train = torch.tensor(alpha_sequences_tokenized[:, 1:], dtype=torch.long).to(device)   # Target sequences\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")"
      ],
      "metadata": {
        "id": "k5N-k1tyv93p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "# Define model ID (use the same one used for saving)\n",
        "# seriestype = \"copper_prices-\"\n",
        "seriestype = \"corn_prices-\"\n",
        "modelid = seriestype + f\"A-{embed_dim}-{num_heads}-{ff_hidden_dim}-{num_layers}\"\n",
        "\n",
        "# Define base folder\n",
        "base_folder = \"/content/drive/MyDrive/Colab Notebooks/AlphaTransformer/\"\n",
        "model_name = f\"transformer_alpha_model_{modelid}\"\n",
        "model_name"
      ],
      "metadata": {
        "id": "PB4-ht3FT1zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define base folder\n",
        "base_folder = \"/content/drive/MyDrive/\"\n",
        "project_folder = base_folder + \"Colab Notebooks/AlphaTransformer/\"\n",
        "model_name = f\"transformer_alpha_model_{modelid}\"\n",
        "\n",
        "# File paths\n",
        "model_path = os.path.join(project_folder, f\"{model_name}.pth\")\n",
        "params_json_path = os.path.join(project_folder, f\"{model_name}_params.json\")\n",
        "params_txt_path = os.path.join(project_folder, f\"{model_name}_params.txt\")"
      ],
      "metadata": {
        "id": "u6rMD_YXSZHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learn_rate = 0.0001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learn_rate) # good lr = 0.0001\n",
        "\n",
        "losses = []\n",
        "num_epochs = 10000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(X_train, X_train)  # Output shape: (batch, seq_len, vocab_size)\n",
        "\n",
        "    # Reshape for loss function: (batch*seq_len, vocab_size) vs. (batch*seq_len,)\n",
        "    loss = criterion(output.view(-1, vocab_size), y_train.view(-1))\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "        # Save model weights\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.yscale('log')\n",
        "\n",
        "#fig_path = os.path.join(base_folder, f\"{model_name}_plot.png\")\n",
        "#plt.savefig(fig_path, dpi=200)\n",
        "plt.show()\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "id": "IdrhKS1IwABq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8\n",
        "import json\n",
        "# Define model and training parameters\n",
        "model_params = {\n",
        "    \"alpha embedd dim\": dimension,\n",
        "    \"alpha lag\": lag,\n",
        "    \"embed_dim\": embed_dim,\n",
        "    \"num_heads\": num_heads,\n",
        "    \"ff_hidden_dim\": ff_hidden_dim,\n",
        "    \"num_layers\": num_layers,\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"sequence_length\": sequence_length,\n",
        "    \"num_training_examples\": len(alpha_sequences),\n",
        "    \"learning_rate\": learn_rate,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"training_data_type\": \"megatrends_copper_prices\",\n",
        "    \"description\": \"Transformer trained to predict alpha sequences from corn price series.\"\n",
        "}\n",
        "\n",
        "# Save as JSON\n",
        "with open(params_json_path, \"w\") as json_file:\n",
        "    json.dump(model_params, json_file, indent=4)\n",
        "\n",
        "# Save as TXT\n",
        "with open(params_txt_path, \"w\") as txt_file:\n",
        "    for key, value in model_params.items():\n",
        "        txt_file.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "print(f\"Model parameters saved as JSON: {params_json_path}\")\n",
        "print(f\"Model parameters saved as TXT: {params_txt_path}\")"
      ],
      "metadata": {
        "id": "W9JU43VLOPmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate a new Brownian motion path for testing\n",
        "# test_series = np.cumsum(np.random.normal(0, 1, sequence_length*2))\n",
        "## Generate a logistic map trajectory for testing\n",
        "# test_series = [random.uniform(0, 1)]  # Random initial value\n",
        "# for _ in range(sequence_length - 1): test_series.append(4 * test_series[-1] * (1 - test_series[-1]))\n",
        "## get the last L values of seq_array for testing\n",
        "# test_series = test_series[-(sequence_length//4):]\n",
        "test_series = seq_array[-sequence_length:]\n",
        "test_embedding = TimeSeriesEmbeddingAlpha(test_series, dimension, lag)\n",
        "\n",
        "test_seq = test_embedding.alpha_sequence  # Extract alpha sequence\n",
        "\n",
        "# Convert test sequence to token indices\n",
        "test_seq_tokenized = np.vectorize(token_map.get)(test_seq)\n",
        "\n",
        "# Convert to tensor\n",
        "test_seq_tensor = torch.tensor(test_seq_tokenized[:-1], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "# Predict next step\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted_logits = model(test_seq_tensor, test_seq_tensor)\n",
        "    predicted_tokens = torch.argmax(predicted_logits, dim=-1)  # Get most probable token indices\n",
        "\n",
        "# Convert predicted indices back to original signed values\n",
        "predicted_tokens_signed = np.vectorize(inverse_token_map.get)(predicted_tokens.squeeze().cpu().numpy())\n",
        "\n",
        "print(\"Input Alpha Sequence (Original):\", test_seq[:-1])\n",
        "print(\"Predicted Next Tokens (Restored Signed Values):\", predicted_tokens_signed)"
      ],
      "metadata": {
        "id": "HDYTEhedwVG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Generate a timestamp\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")  # Format: YYYYMMDD-HHMMSS\n",
        "\n",
        "# Define save path for the figure with timestamp\n",
        "fig_path = os.path.join(project_folder, f\"{model_name}_plot_{timestamp}.png\")\n",
        "\n",
        "# Define save path for the figure\n",
        "# fig_path = os.path.join(base_folder, f\"{model_name}_plot.png\")"
      ],
      "metadata": {
        "id": "qjz4kjgjZ_NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plot original vs. predicted sequences\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original sequence (blue circles)\n",
        "plt.plot(range(len(test_seq[:-1])), test_seq[:-1], 'bo-', label='Original Sequence', markersize=8)\n",
        "\n",
        "# Predicted sequence (red triangles)\n",
        "plt.plot(range(len(predicted_tokens_signed)), predicted_tokens_signed, 'r^-', label='Predicted Sequence', markersize=8)\n",
        "\n",
        "# Labels & Legend\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Alpha Sequence Value\")\n",
        "plt.title(\"Original vs. Predicted Alpha Sequence\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "# Save figure with 200 dpi\n",
        "plt.savefig(fig_path, dpi=200)\n",
        "print(f\"Figure saved to {fig_path}\")\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sn6OYtjx1Y-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict last m tokens"
      ],
      "metadata": {
        "id": "Zn8bFNLe2W6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m = 1  # Number of tokens to predict\n",
        "\n",
        "# test_embedding = TimeSeriesEmbeddingAlpha(test_series, dimension, lag)\n",
        "# test_seq = test_embedding.alpha_sequence  # Extract alpha sequence\n",
        "\n",
        "# # Convert test sequence to token indices\n",
        "# test_seq_tokenized = np.vectorize(token_map.get)(test_seq)\n",
        "\n",
        "# Split input and target sequences\n",
        "test_seq_input = test_seq_tokenized[:-m]  # Input sequence (excluding last `m` tokens)\n",
        "test_seq_target = test_seq_tokenized[-m:]  # Ground truth for last `m` tokens\n",
        "\n",
        "# Convert to tensor\n",
        "test_seq_tensor = torch.tensor(test_seq_input, dtype=torch.long).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
        "\n",
        "# Predict `m` tokens\n",
        "model.eval()\n",
        "predicted_tokens = []\n",
        "with torch.no_grad():\n",
        "    for _ in range(m):\n",
        "        # Forward pass\n",
        "        predicted_logits = model(test_seq_tensor, test_seq_tensor)  # Output shape: (1, seq_len, vocab_size)\n",
        "\n",
        "        # Extract the last predicted token\n",
        "        next_token = torch.argmax(predicted_logits[:, -1, :], dim=-1)  # Shape: (1,)\n",
        "\n",
        "        # Reshape `next_token` to match `test_seq_tensor`\n",
        "        next_token = next_token.unsqueeze(1)  # Shape: (1, 1) -> (batch, seq_len)\n",
        "\n",
        "        # Append predicted token to input sequence (autoregressive modeling)\n",
        "        test_seq_tensor = torch.cat([test_seq_tensor, next_token], dim=1)  # Concatenates along sequence dimension\n",
        "\n",
        "        # Store predicted token\n",
        "        predicted_tokens.append(next_token.item())\n",
        "\n",
        "# Convert predicted indices back to original signed values\n",
        "predicted_tokens_signed = np.vectorize(inverse_token_map.get)(predicted_tokens)\n",
        "\n",
        "# Convert ground truth back to signed values\n",
        "test_seq_target_signed = np.vectorize(inverse_token_map.get)(test_seq_target)\n",
        "\n",
        "print(\"Input Alpha Sequence (Original, Excluding Last m): \\n\", test_seq[:-m], \"\\n\")\n",
        "print(\"Ground Truth Last m Tokens: \\n\", test_seq_target_signed, \"\\n\")\n",
        "print(\"Predicted Last m Tokens:\\n\", predicted_tokens_signed, \"\\n\")"
      ],
      "metadata": {
        "id": "6DXEKZNt7KZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create x-axis positions\n",
        "x_full = np.arange(len(test_seq))  # Full sequence indices\n",
        "x_original = x_full[:-m]  # Before the last m tokens\n",
        "x_ground_truth = x_full[-m:]  # Last m tokens (ground truth)\n",
        "x_predicted = x_ground_truth  # Last m tokens (predictions)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original sequence before last m points (blue circles)\n",
        "plt.plot(x_original, test_seq[:-m], 'bo-', label='Original Sequence', markersize=8)\n",
        "\n",
        "# Last m points of original sequence  (crosses)\n",
        "plt.plot(x_ground_truth, test_seq_target_signed, 'x--', color='blue', label='Ground Truth (Last m)', markersize=6)\n",
        "\n",
        "# Predicted sequence (red triangles)\n",
        "plt.plot(x_predicted, predicted_tokens_signed, 'r^--', label='Predicted token', markersize=4)\n",
        "\n",
        "# Add dashed blue line: Connect last original point to first ground truth point\n",
        "plt.plot([x_original[-1], x_ground_truth[0]], [test_seq[:-m][-1], test_seq_target_signed[0]], 'b--')\n",
        "\n",
        "# Add dashed red line: Connect last original point to first predicted point\n",
        "plt.plot([x_original[-1], x_predicted[0]], [test_seq[:-m][-1], predicted_tokens_signed[0]], 'r--')\n",
        "\n",
        "# Labels & Legend\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Alpha Sequence Value\")\n",
        "plt.title(f\"Original vs. Predicted Alpha Sequence (Predicting Last {m} Tokens)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "fig_path = os.path.join(project_folder, f\"{model_name}_predict_{m}_last_.png\")\n",
        "plt.savefig(fig_path, dpi=200)\n",
        "print(f\"Figure saved to {fig_path}\")\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SiX8ihEBFPro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_array[-9:]"
      ],
      "metadata": {
        "id": "2YKvIhFHTQiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot seq_array[-9:]\n",
        "\n",
        "plt.plot(seq_array[-8:-1], 'bo-')\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Last 7 Values of seq_array before predicting the alpha value consistent with the past\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SuaShfWWZFum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argsort(np.argsort(seq_array[-8:-1]))"
      ],
      "metadata": {
        "id": "Yul3PitQRq-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embdd = TimeSeriesEmbeddingAlpha(seq_array[-10:], dimension, lag)\n",
        "embdd.alpha_sequence"
      ],
      "metadata": {
        "id": "1V6LZ2pLTA_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create x-axis positions\n",
        "x_full = np.arange(len(test_seq))\n",
        "x_original = x_full[:-m]  # Before the last m tokens\n",
        "x_ground_truth = x_full[-m:]  # Last m tokens\n",
        "x_predicted = x_ground_truth  # Predicted points align with last m tokens\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original sequence (blue circles, excluding last m points)\n",
        "plt.plot(x_original, test_seq[:-m], 'bo-', label='Original Sequence', markersize=8)\n",
        "\n",
        "# Last m points of original sequence (orange crosses)\n",
        "plt.plot(x_ground_truth, test_seq_target_signed, 'x-', color='blue', label='Ground Truth (Last m)', markersize=10)\n",
        "\n",
        "# Predicted sequence (red triangles)\n",
        "plt.plot(x_predicted, predicted_tokens_signed, 'r^-', label='Predicted Sequence', markersize=8)\n",
        "\n",
        "# Labels & Legend\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Alpha Sequence Value\")\n",
        "plt.title(f\"Original vs. Predicted Alpha Sequence (Predicting Last {m} Tokens)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8GeZQoQw7d9f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}