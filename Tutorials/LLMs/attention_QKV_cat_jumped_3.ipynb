{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNvqbvqsw+pv0N6HDUkTjwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvaroEkel/AI-Spielplatz/blob/main/attention_QKV_cat_jumped_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TLIAHO8gcf9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the Attention Mechanism\n",
        "\n",
        "This notebook explains the attention mechanism step-by-step using a simplified version of the sentence:\n",
        "\n",
        "> **\"The cat, which had been sleeping on the couch, suddenly jumped.\"**\n",
        "\n",
        "We'll focus on how the word **\"jumped\"** attends to other words using the Query-Key-Value mechanism."
      ],
      "metadata": {
        "id": "-wNyW2sYchBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "pu56dgfsccQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified example with toy vectors\n",
        "words = [\"cat\", \"sleeping\", \"couch\", \"suddenly\", \"jumped\"]\n",
        "vectors = {\n",
        "    \"cat\": np.array([1.0, 0.0]),\n",
        "    \"sleeping\": np.array([0.8, 0.3]),\n",
        "    \"couch\": np.array([0.4, 0.6]),\n",
        "    \"suddenly\": np.array([0.1, 0.9]),\n",
        "    \"jumped\": np.array([0.9, 0.1])\n",
        "}\n",
        "\n",
        "# Use the vector for 'jumped' as the Query\n",
        "q = vectors[\"jumped\"]\n",
        "\n",
        "# Compute dot products between Query and each Key: Q*K^T\n",
        "scores = np.array([np.dot(q, vectors[w])/np.sqrt(2) for w in words])\n",
        "# scores = np.array([np.dot(q, vectors[w]) for w in words])\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "# Compute attention weights: softmax(Q/)\n",
        "weights = softmax(scores)\n",
        "\n",
        "# Compute the attended output vector\n",
        "attended_output = sum(weights[i] * vectors[words[i]] for i in range(len(words)))\n",
        "# print the attended output\n",
        "print(f\"Attended output vector: {attended_output}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pzcwXOTKckwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the vectors\n",
        "fig, ax = plt.subplots()\n",
        "for word, vec in vectors.items():\n",
        "    ax.arrow(0, 0, vec[0], vec[1], head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
        "    ax.text(vec[0]*1.15, vec[1]*1.15, word)\n",
        "    ax.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mxeiRqngcmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot attention weights\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(words, weights, color='skyblue')\n",
        "plt.title(\"Attention Weights from 'jumped'\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Show weights\n",
        "print(f\"Query = '{words[-1]}'\")\n",
        "for word, w in zip(words, weights):\n",
        "    print(f\"{word:>10}: {w:.3f}\")\n"
      ],
      "metadata": {
        "id": "HSmj0imOcq-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYRedIGsdjhC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}