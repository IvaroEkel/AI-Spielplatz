{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqqOQez47vxh9mJyqGX6gS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvaroEkel/AI-Spielplatz/blob/main/Tutorials/LLMs/Cloud/1.0-Intrduction_and_Contents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications of Large Language Models\n",
        "\n",
        "## Contents of this tutorial\n",
        "\n",
        "*   1.1 Installing Ollama\n",
        "*   1.2 Summarization\n"
      ],
      "metadata": {
        "id": "dUsk0OFZ_xds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are LLMs?\n",
        "\n",
        "LLMs are Artificial Intelligence (AI) models based on the Transformer Neural Network architechture. This architechture and its applications on building Generative AI and other Machine Learning methods.\n",
        "\n",
        "A Transformer is an Artificial Neural Network (ANN) that consists of layers that process input data (encoding or encoder layers, or simply encoders) and layers that generate the ouptut (decoders).\n",
        "\n",
        "### Encoder-only models\n",
        "\n",
        "These models focus exclusively on analyzing input data. An early example of these models is BERT (Bidirectional encoder representations from transformers). Encoder-only models are extensively used for task where processing a text for tasks such as sentiment analysis and question answering.  \n",
        "\n",
        "### Decoder-only models\n",
        "\n",
        "These models specialize on text generation based on an input (commonly known now as \"prompt\"). The most popular representative of this family of models is perhaps the series of GPT-X models. For instance, GPT-3 uses an approach called \"autoregressive language modeling. This consists on updating the knowledge acquired after each prediction, improving the predictions as they come.\n",
        "\n",
        "### Encoder-Decoder models\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Text-to-text transfer transformer (T5). Multipurpose and versatile.\n",
        "* Meta Llama 3.X series\n",
        "\n",
        "### Reducing computational load of LLMs\n",
        "\n",
        "#### Model distillation and quantization.\n",
        "Facilitates the deployment these powerful and heavy models in production environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "IzbNrpuzJWxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "hJOF5CUHGH5P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X7PSd_PmGJ01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}