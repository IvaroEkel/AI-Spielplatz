{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqAU0JVE/5575Xl2h3GdXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvaroEkel/AI-Spielplatz/blob/main/Tutorials/LLMs/Cloud/1.0-Intrduction_and_Contents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications of Large Language Models\n",
        "\n",
        "## Contents of this tutorial\n",
        "\n",
        "*   1.1 Installing Ollama\n",
        "*   1.2 Summarization\n"
      ],
      "metadata": {
        "id": "dUsk0OFZ_xds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are LLMs?\n",
        "\n",
        "LLMs are Artificial Intelligence (AI) models based on the Transformer Neural Network architechture. This architechture and its applications on building Generative AI and other Machine Learning methods.\n",
        "\n",
        "A Transformer is an Artificial Neural Network (ANN) that consists of layers that process input data (encoding or encoder layers, or simply encoders) and layers that generate the ouptut (decoders).\n",
        "\n",
        "### Encoder-only models\n",
        "\n",
        "These models focus exclusively on analyzing input data. An early example of these models is BERT (Bidirectional encoder representations from transformers). Encoder-only models are extensively used for task where processing a text for tasks such as sentiment analysis and question answering.  \n",
        "\n",
        "### Decoder-only models\n",
        "\n",
        "These models specialize on text generation based on an input (commonly known now as \"prompt\"). The most popular representative of this family of models is perhaps the series of GPT-X models. For instance, GPT-3 uses an approach called \"autoregressive language modeling. This consists on updating the knowledge acquired after each prediction, improving the predictions as they come.\n",
        "\n",
        "### Encoder-Decoder models\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Text-to-text transfer transformer (T5). Multipurpose and versatile.\n",
        "* Meta Llama 3.X series\n",
        "\n",
        "### Reducing computational load of LLMs\n",
        "\n",
        "#### Model distillation and quantization.\n",
        "Facilitates the deployment these powerful and heavy models in production environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "IzbNrpuzJWxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify your task. It's fundamental.\n",
        "\n",
        "To choose the most suitable LLM for your task, begin by identifying the task's natureâ€”is it focused on understanding, generating, or both? If the goal is text understanding or analysis, encoder-only models might be sufficient. For content generation, decoder-only models are typically more effective. For tasks that require both, encoder-decoder models are a better fit. Then, assess whether existing models fulfill your requirements or if fine-tuning is needed. For specialized tasks involving your specific data, fine-tuning may be crucial. Developing an LLM from the ground up is highly expensive, demanding significant investments in computation, data, and expertise. As a result, utilizing existing models and prioritizing fine-tuning or prompt engineering is usually a more practical approach.\n"
      ],
      "metadata": {
        "id": "grm9NQS-J5a_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of fine-tunable model: FLAN T5\n",
        "\n",
        "FLAN T5 enhances T5 by training T5 with a technique called instruction tunning that consistst in using a wide variety of instructions instead of a single set of specific instructions. This allows it to be fine tuned to generic tasks without the need to be retrained from scratch."
      ],
      "metadata": {
        "id": "fiXkp9m1L79A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering. A real and serious task, but not an Engineering.\n",
        "\n",
        "In this context, a *prompt* is an instruction or set of instructions for the LLM to perform. It is necessary to provide the LLM with a task and sometimes\n",
        "the necessary context of that task.\n",
        "\n",
        "Prompt engineering refers to the crafting of crafting the right prompts the get the best performance out of an LLM.\n",
        "\n",
        "Therefore, standardized approaches to build prompts are fundamental to help recognize the model recognize the task appropriately and do the corresponding processing and output, thus enhancing model performance.\n",
        "\n",
        "## Main prompt engineering patterns\n",
        "\n",
        "### 1 - Few-shot pattern\n",
        "This prompt pattern involves showing the model multiple examples of a task before asking it to solve a new instance. For instance, if the goal is to train the model to recognize animal names in a text, you would start by providing examples with labeled animals. For example, given the prompt: \"The quick brown fox jumps over the lazy dog,\" you would label \"fox\" and \"dog\" as the identified animals. Then, you would present the actual prompt: \"A sheep and a wolf became unlikely friends.\" By doing this, the model gains a clearer understanding of the expected output format when tasked with identifying animals.\n",
        "\n",
        "### 2 - Cognitive verifier\n",
        "\n",
        " This approach is particularly valuable when it's essential to ensure a thorough and accurate approach to a subject, especially if there's uncertainty about covering all relevant aspects. This pattern enhances the reliability of the outputs by prompting the LLM to verify and clarify necessary information before delivering a final response. It is especially effective when dependable and accurate information is crucial for problem-solving. For example, a suitable prompt to initiate this process could be: \"Whenever I ask a question, first request any additional information needed to clarify my query before providing a final answer. Please incorporate all my responses.\" Such a prompt encourages the model to generate follow-up questions, refining the understanding of the request. I recommend experimenting with this in your preferred chatbot to see its effectiveness.\n",
        "\n",
        "### 3 - Question refinement\n",
        "\n",
        "The final approach is the question refinement pattern, which encourages the LLM to clarify or elaborate on a query before responding. This involves the model posing additional questions to gather more context or details, enabling it to deliver a more precise and relevant answer. A typical opening prompt for this method might be: \"When I ask a question, suggest a better question and ask me if I would like to use it instead\". By experimenting with these patterns, you'll discover how much the phrasing of a prompt can influence the results you achieve with an LLM. Continuously test different techniques, adjust your prompts based on the responses, and refine your approach through ongoing interaction.\n",
        "\n",
        "\n",
        "### 4 - Gameplay pattern\n",
        "\n",
        "ChatGPT\n",
        "\n",
        "The gameplay pattern fosters curiosity and motivation by incorporating interactive, game-like elements into the experience with AI. It uses challenges, strategic thinking, or playful interactions to enhance learning on a specific topic or to refine a creative output.\n",
        "\n"
      ],
      "metadata": {
        "id": "EKsOOs-XM-Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfmu-_SNL7VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliography & Resources"
      ],
      "metadata": {
        "id": "hJOF5CUHGH5P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X7PSd_PmGJ01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}